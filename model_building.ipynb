{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4768d037-395b-4181-83f5-a6938d09bd5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch # Importing PyTorch for deep learning tasks\n",
    "import torch.nn as nn # Importing neural network module from PyTorch\n",
    "import torch.optim as optim # Importing optimization algorithms from PyTorch\n",
    "from torch.utils.data import DataLoader, Dataset # Importing DataLoader and Dataset classes for handling data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random #\n",
    "import nltk # Importing Natural Language Toolkit for text processing\n",
    "import pickle # Importing pickle for saving and loading Python objects\n",
    "from nltk.corpus import wordnet # Importing WordNet corpus from NLTK\n",
    "from sklearn.preprocessing import LabelEncoder # Importing LabelEncoder for encoding labels\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import classification_report\n",
    "from collections import defaultdict # Importing defaultdict for creating dictionaries with default values\n",
    "import streamlit as st \n",
    "import re # Importing regular expressions for text processing\n",
    "from collections import Counter # Importing Counter for counting hashable objects\n",
    "\n",
    "nltk.download('wordnet') # Downloading WordNet corpus for NLTK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c365b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 100        # Maximum length of input sequences\n",
    "BATCH_SIZE = 32      # Batch size for training\n",
    "EPOCHS = 2           # Number of training epochs\n",
    "EMBED_DIM = 128      # Dimension of word embeddings\n",
    "HIDDEN_DIM = 64      # Dimension of LSTM hidden layers\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # Hardware selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62f298c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMWithAttention(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim):\n",
    "        super(BiLSTMWithAttention, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.attention = nn.Linear(hidden_dim * 2, 1)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        outputs, _ = self.lstm(embedded)\n",
    "        attn_weights = torch.softmax(self.attention(outputs).squeeze(-1), dim=1)\n",
    "        context = torch.sum(outputs * attn_weights.unsqueeze(-1), dim=1)\n",
    "        return self.fc(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73acae23",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Key Features:**\n",
    "- **Embedding Layer:** Converts word indices to dense vectors.\n",
    "- **Bidirectional LSTM:** Captures context from both directions in the sequence.\n",
    "- **Attention Mechanism:** Learns to focus on the most relevant words for emotion detection.\n",
    "- **Fully Connected Layer:** Outputs class scores for each emotion.\n",
    "\n",
    "---\n",
    "\n",
    "## 7. **Training and Evaluation**\n",
    "\n",
    "- **Training Loop:**  \n",
    "  The `train` function performs forward and backward passes, updating model weights and tracking loss.\n",
    "\n",
    "- **Evaluation:**  \n",
    "  The `evaluate` function computes predictions on the validation set and prints a detailed classification report (precision, recall, F1-score) for each emotion class.\n",
    "\n",
    "- **Epochs:**  \n",
    "  The model is trained for the specified number of epochs, with performance metrics displayed after each epoch.\n",
    "\n",
    "---\n",
    "\n",
    "## 8. **Saving Model and Artifacts**\n",
    "\n",
    "After training, the following artifacts are saved for deployment:\n",
    "\n",
    "- **Model Weights:** bilstm_model.pt\n",
    "- **Vocabulary:** vocab.pkl\n",
    "- **Label Encoder:** label_encoder.pkl\n",
    "\n",
    "These files are essential for inference and integration with applications such as Streamlit.\n",
    "\n",
    "---\n",
    "\n",
    "## 9. **Integration and Deployment**\n",
    "\n",
    "- The saved model and assets can be loaded in a Streamlit app (app.py or new.py) for real-time emotion detection and recommendations.\n",
    "- The modular design allows easy extension to other NLP tasks or integration with external APIs.\n",
    "\n",
    "---\n",
    "\n",
    "## 10. **Best Practices and Recommendations**\n",
    "\n",
    "- **Data Quality:** Ensure cleaned_data.csv is properly preprocessed for optimal model performance.\n",
    "- **Hyperparameter Tuning:** Experiment with `EMBED_DIM`, `HIDDEN_DIM`, and `EPOCHS` for best results.\n",
    "- **Model Interpretability:** The attention mechanism provides insights into which words influence predictions.\n",
    "- **Scalability:** The script is designed to handle large datasets efficiently using PyTorch’s DataLoader.\n",
    "\n",
    "---\n",
    "\n",
    "## 11. **Conclusion**\n",
    "\n",
    "This script forms the backbone of the Vibe Bot’s emotion classification engine. It demonstrates best practices in NLP preprocessing, deep learning model construction, and artifact management for deployment. The approach is robust, scalable, and ready for integration into production systems.\n",
    "\n",
    "---\n",
    "\n",
    "**For further details or live demonstrations, refer to the Streamlit application and associated deployment scripts in the project directory.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a437b1d0-1ace-4505-b407-4bbf70b55dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "Train Loss: 1.7640\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry       0.50      0.63      0.56       875\n",
      "    confused       0.73      0.54      0.62       511\n",
      "   depressed       0.45      0.65      0.53      1109\n",
      "     excited       0.39      0.39      0.39       534\n",
      "        fear       0.82      0.73      0.77       873\n",
      "    grateful       0.30      0.22      0.25       502\n",
      "       happy       0.69      0.25      0.37       296\n",
      "     hopeful       0.61      0.73      0.67      1124\n",
      "      lonely       0.53      0.67      0.59       987\n",
      "      regret       0.73      0.28      0.40       573\n",
      "    rejected       0.93      0.42      0.58       303\n",
      "         sad       0.34      0.29      0.31       965\n",
      "\n",
      "    accuracy                           0.53      8652\n",
      "   macro avg       0.59      0.48      0.50      8652\n",
      "weighted avg       0.56      0.53      0.53      8652\n",
      "\n",
      "Epoch 2/2\n",
      "Train Loss: 1.2542\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry       0.57      0.62      0.59       875\n",
      "    confused       0.71      0.58      0.64       511\n",
      "   depressed       0.49      0.65      0.56      1109\n",
      "     excited       0.42      0.49      0.45       534\n",
      "        fear       0.86      0.70      0.77       873\n",
      "    grateful       0.39      0.35      0.37       502\n",
      "       happy       0.73      0.31      0.44       296\n",
      "     hopeful       0.75      0.70      0.73      1124\n",
      "      lonely       0.58      0.66      0.62       987\n",
      "      regret       0.62      0.42      0.50       573\n",
      "    rejected       0.86      0.59      0.70       303\n",
      "         sad       0.34      0.40      0.37       965\n",
      "\n",
      "    accuracy                           0.57      8652\n",
      "   macro avg       0.61      0.54      0.56      8652\n",
      "weighted avg       0.60      0.57      0.57      8652\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ========== CONFIG ========== #\n",
    "MAX_LEN = 100 # Maximum length of input sequences\n",
    "BATCH_SIZE = 32 # Batch size for training\n",
    "EPOCHS = 2 # Number of epochs for training\n",
    "EMBED_DIM = 128 # Dimension of word embeddings\n",
    "HIDDEN_DIM = 64 # Dimension of hidden layers in LSTM\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ========== DATA LOADING ========== #\n",
    "df = pd.read_csv(\"cleaned_data.csv\")  # Ensure this has 'text' and 'label'\n",
    "le = LabelEncoder() #   LabelEncoder to convert string labels to integers\n",
    "df['emotion'] = le.fit_transform(df['emotion']) # Encode the 'emotion' column\n",
    "\n",
    "# Train-validation split\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    df['cleaned_text'], df['emotion'], test_size=0.2, stratify=df['emotion'], random_state=42)\n",
    "\n",
    "# Build vocab without torchtext\n",
    "vocab_dict = {'<pad>': 0, '<unk>': 1} # Initialize vocabulary dictionary with padding and unknown tokens\n",
    "index = 2 # Start indexing from 2 to reserve 0 for padding and 1 for unknown\n",
    "\n",
    "def tokenize(text):\n",
    "    return re.findall(r\"\\b\\w+\\b\", text.lower()) # Tokenize text into words, converting to lowercase\n",
    "\n",
    "for text in train_texts:\n",
    "    for token in tokenize(text):\n",
    "        if token not in vocab_dict:\n",
    "            vocab_dict[token] = index\n",
    "            index += 1\n",
    "\n",
    "# Text to indices\n",
    "def text_to_sequence(text):\n",
    "    tokens = tokenize(text)\n",
    "    ids = [vocab_dict.get(token, vocab_dict['<unk>']) for token in tokens[:MAX_LEN]]\n",
    "    padded = ids + [vocab_dict['<pad>']] * (MAX_LEN - len(ids))\n",
    "    return padded\n",
    "\n",
    "X_train = torch.tensor([text_to_sequence(text) for text in train_texts]) # Convert training texts to sequences\n",
    "X_val = torch.tensor([text_to_sequence(text) for text in val_texts]) # Convert validation texts to sequences\n",
    "y_train = torch.tensor(train_labels.tolist()) # Convert training labels to tensor\n",
    "y_val = torch.tensor(val_labels.tolist()) # Convert validation labels to tensor\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train, y_train) # Create TensorDataset for training data\n",
    "val_dataset = torch.utils.data.TensorDataset(X_val, y_val) # Create TensorDataset for validation data\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True) # Create DataLoader for training data\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE) # Create DataLoader for validation data\n",
    "\n",
    "# ========== MODEL ========== #\n",
    "class BiLSTMWithAttention(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim):\n",
    "        super(BiLSTMWithAttention, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.attention = nn.Linear(hidden_dim * 2, 1)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        outputs, _ = self.lstm(embedded)\n",
    "        attn_weights = torch.softmax(self.attention(outputs).squeeze(-1), dim=1)\n",
    "        context = torch.sum(outputs * attn_weights.unsqueeze(-1), dim=1)\n",
    "        return self.fc(context)\n",
    "\n",
    "model = BiLSTMWithAttention(len(vocab_dict), embed_dim=EMBED_DIM, hidden_dim=HIDDEN_DIM, output_dim=len(le.classes_)).to(DEVICE)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# ========== TRAINING LOOP ========== #\n",
    "def train(model, loader):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for inputs, labels in loader:\n",
    "        inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(inputs)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    print(classification_report(all_labels, all_preds, target_names=le.classes_))\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    loss = train(model, train_loader)\n",
    "    print(f\"Train Loss: {loss:.4f}\")\n",
    "    evaluate(model, val_loader)\n",
    "\n",
    "# Save model and vocab\n",
    "torch.save(model.state_dict(), \"bilstm_model.pt\")\n",
    "with open(\"vocab.pkl\", \"wb\") as f:\n",
    "    pickle.dump(vocab_dict, f)\n",
    "with open(\"label_encoder.pkl\", \"wb\") as f:\n",
    "    pickle.dump(le, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb56233-a9c8-4d51-90a2-e175e119b273",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d11d8b2-66b8-4baf-8488-34ac4b8f88a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
