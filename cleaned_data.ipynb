{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b9e30d7-b386-45dc-8e63-abf89d8b12a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re  #Regular expressions: Used for pattern matching (e.g., removing URLs, punctuation, hashtags).\n",
    "import emoji  #Used to detect and remove emojis from text.\n",
    "import nltk   #The Natural Language Toolkit for tokenization, lemmatization, and stopwords.\n",
    "import pandas as pd\n",
    "import csv    ##Handles CSV file operations, especially with quoting.\n",
    "from nltk.corpus import stopwords  #Imports the list of stopwords (e.g., \"and\", \"the\", \"is\")\n",
    "from nltk.stem import WordNetLemmatizer  #Imports the WordNet-based lemmatizer for reducing words to their base form (e.g., \"running\" → \"run\").\n",
    "from textblob import TextBlob   #Used here for optional spelling correction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8045c3f5-0945-4e9d-96bd-f837acb6c3ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final cleaned file saved as 'cleaned_data.csv'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Download NLTK data\n",
    "nltk.download('punkt')     #Sentence and word tokenizer.\n",
    "nltk.download('wordnet')   #Lexical database used for lemmatization.\n",
    "nltk.download('stopwords') #Common words (like \"the\", \"is\", \"in\") which are often removed.\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()  #A lemmatizer (to reduce words to their base form, e.g., \"running\" → \"run\").\n",
    "stop_words = set(stopwords.words('english')) #A set of stopwords to filter out common words.\n",
    "\n",
    "# Contraction map\n",
    "contractions_dict = {\n",
    "    \"can't\": \"cannot\", \"won't\": \"will not\", \"i'm\": \"i am\", \"it's\": \"it is\",\n",
    "    \"don't\": \"do not\", \"didn't\": \"did not\", \"you're\": \"you are\", \"i've\": \"i have\",\n",
    "    \"they're\": \"they are\", \"that's\": \"that is\", \"isn't\": \"is not\", \"aren't\": \"are not\",\n",
    "    \"wasn't\": \"was not\", \"weren't\": \"were not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
    "    \"couldn't\": \"could not\", \"shouldn't\": \"should not\", \"wouldn't\": \"would not\",\n",
    "    \"there's\": \"there is\", \"what's\": \"what is\", \"who's\": \"who is\", \"let's\": \"let us\",\n",
    "    \"he's\": \"he is\", \"she's\": \"she is\", \"we're\": \"we are\"\n",
    "}\n",
    "contractions_pattern = re.compile(r'\\b(' + '|'.join(re.escape(k) for k in contractions_dict) + r')\\b')\n",
    "#A dictionary to expand contractions (e.g., \"can't\" → \"cannot\").\n",
    "#Compiles a regex pattern to identify them.\n",
    "\n",
    "def expand_contractions(text):\n",
    "    return contractions_pattern.sub(lambda x: contractions_dict[x.group()], text)\n",
    "    #Function to replace contractions in a given text.\n",
    "\n",
    "def remove_emojis(text):\n",
    "    return emoji.replace_emoji(text, replace='')\n",
    "    #Removes emojis using the emoji library.\n",
    "\n",
    "def clean_text_emotion(text, correct_spelling=False, remove_stopwords=False,\n",
    "                       min_word_count=3, min_char_length=10):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    text = text.lower()  #Convert to lowercase\n",
    "    text = expand_contractions(text)  #Expand contractions\n",
    "    text = remove_emojis(text)    #Remove emojis\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)  #Remove URLs digits\n",
    "    text = re.sub(r\"@\\w+|#\\w+\", \"\", text)    #mentions (@user), hashtags\n",
    "    text = re.sub(r\"[^\\w\\s!?]\", \"\", text)  # remove punctuation\n",
    "    text = re.sub(r\"\\d+\", \"\", text)        #remove digits\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    if correct_spelling:\n",
    "        text = str(TextBlob(text).correct())\n",
    "        #Optional spelling correction using TextBlob\n",
    "\n",
    "    words = nltk.word_tokenize(text)  #Tokenize words\n",
    "    clean_words = []\n",
    "\n",
    "    #Remove stopwords (if enabled)\n",
    "    for word in words:\n",
    "        if remove_stopwords and word in stop_words:\n",
    "            continue\n",
    "        lemma = lemmatizer.lemmatize(word)  #Lemmatize words\n",
    "        clean_words.append(lemma)\n",
    "\n",
    "    cleaned_text = \" \".join(clean_words)  #Joins words back into a sentence\n",
    "\n",
    "    if len(clean_words) < min_word_count or len(cleaned_text) < min_char_length:\n",
    "        return \"\"  #Returns an empty string if the cleaned text is too short\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "# Load CSV safely\n",
    "df = pd.read_csv(\"merged_data.csv\", quoting=csv.QUOTE_ALL, encoding='utf-8', low_memory=False)\n",
    "\n",
    "# Clean text\n",
    "df[\"cleaned_text\"] = df[\"text\"].apply(lambda x: clean_text_emotion(x, correct_spelling=False, remove_stopwords=True))\n",
    "\n",
    "# Drop empty rows\n",
    "df = df[df[\"cleaned_text\"].str.strip() != \"\"] #Removes rows where cleaned text is empty\n",
    "\n",
    "df.drop(['subreddit','text'],inplace=True,axis=1)\n",
    "# Save cleaned data\n",
    "df.to_csv(\"cleaned_data.csv\", index=False, quoting=csv.QUOTE_ALL, encoding='utf-8')\n",
    "print(\"Final cleaned file saved as 'cleaned_data.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa8b92fb-abff-4b78-9666-e17b2e228fab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>happy</td>\n",
       "      <td>got bird back ! ! ! missing month shes back ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>happy</td>\n",
       "      <td>actually happening maternity leave start baby ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>happy</td>\n",
       "      <td>past week feeling better hopeful future today ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>happy</td>\n",
       "      <td>posting daily update made happy mate final int...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>happy</td>\n",
       "      <td>people extremely depressed lacking purpose cha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>happy</td>\n",
       "      <td>finally saved surprise girlfriend everything e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>happy</td>\n",
       "      <td>posting daily update made happy caught mate br...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>happy</td>\n",
       "      <td>almost infinite spark year depression tried de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>happy</td>\n",
       "      <td>overcame multitude struggle area today without...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>happy</td>\n",
       "      <td>first ever wooden project success made railing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>happy</td>\n",
       "      <td>posting daily update made happy shift met favo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>happy</td>\n",
       "      <td>im getting bachelor year old dont like braggin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>happy</td>\n",
       "      <td>hike happiness journey destination video expla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>happy</td>\n",
       "      <td>birthday weekend treated pizza ! sometimes sim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>happy</td>\n",
       "      <td>autistic daughter fell asleep hate cuddling cl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   emotion                                       cleaned_text\n",
       "0    happy  got bird back ! ! ! missing month shes back ho...\n",
       "1    happy  actually happening maternity leave start baby ...\n",
       "2    happy  past week feeling better hopeful future today ...\n",
       "3    happy  posting daily update made happy mate final int...\n",
       "4    happy  people extremely depressed lacking purpose cha...\n",
       "5    happy  finally saved surprise girlfriend everything e...\n",
       "6    happy  posting daily update made happy caught mate br...\n",
       "7    happy  almost infinite spark year depression tried de...\n",
       "8    happy  overcame multitude struggle area today without...\n",
       "9    happy  first ever wooden project success made railing...\n",
       "10   happy  posting daily update made happy shift met favo...\n",
       "11   happy  im getting bachelor year old dont like braggin...\n",
       "12   happy  hike happiness journey destination video expla...\n",
       "13   happy  birthday weekend treated pizza ! sometimes sim...\n",
       "14   happy  autistic daughter fell asleep hate cuddling cl..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95dcfd16-7e87-48a5-beaa-a5b33a34abb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['emotion', 'cleaned_text'], dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "036103c7-e11e-441c-9528-6534e3951f90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['happy', 'sad', 'angry', 'fear', 'depressed', 'excited', 'lonely',\n",
       "       'grateful', 'confused', 'regret', 'hopeful', 'rejected'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['emotion'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a033bb1c-5819-4508-b443-6de4d37c99e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "emotion\n",
       "hopeful      5620\n",
       "depressed    5544\n",
       "lonely       4933\n",
       "sad          4825\n",
       "angry        4375\n",
       "fear         4363\n",
       "regret       2863\n",
       "excited      2669\n",
       "confused     2554\n",
       "grateful     2513\n",
       "rejected     1517\n",
       "happy        1480\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['emotion'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8bbbb3-d893-4699-b6c6-20cb83696c94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
