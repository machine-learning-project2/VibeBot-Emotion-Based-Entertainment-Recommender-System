{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "526a7588",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import nltk\n",
    "import pickle\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "nltk.download('wordnet')  # Only needed once\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0fb60367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "MAX_LEN = 100\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 2\n",
    "EMBED_DIM = 128\n",
    "HIDDEN_DIM = 64\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86d7166e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV\n",
    "df = pd.read_csv(\"cleaned_data.csv\")  # Ensure columns: 'cleaned_text' & 'emotion'\n",
    "df = df.dropna(subset=['cleaned_text', 'emotion'])  # Safety\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "df['emotion'] = le.fit_transform(df['emotion'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c654ac5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    df['cleaned_text'], df['emotion'], test_size=0.2, stratify=df['emotion'], random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a950776e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocabulary\n",
    "vocab_dict = {'<pad>': 0, '<unk>': 1}\n",
    "index = 2\n",
    "\n",
    "def tokenize(text):\n",
    "    return re.findall(r\"\\b\\w+\\b\", text.lower())\n",
    "\n",
    "for text in train_texts:\n",
    "    for token in tokenize(text):\n",
    "        if token not in vocab_dict:\n",
    "            vocab_dict[token] = index\n",
    "            index += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebdd9ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text to padded sequences\n",
    "def text_to_sequence(text):\n",
    "    tokens = tokenize(text)\n",
    "    ids = [vocab_dict.get(token, vocab_dict['<unk>']) for token in tokens[:MAX_LEN]]\n",
    "    padded = ids + [vocab_dict['<pad>']] * (MAX_LEN - len(ids))\n",
    "    return padded\n",
    "\n",
    "X_train = torch.tensor([text_to_sequence(text) for text in train_texts])\n",
    "X_val = torch.tensor([text_to_sequence(text) for text in val_texts])\n",
    "y_train = torch.tensor(train_labels.tolist())\n",
    "y_val = torch.tensor(val_labels.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "75ec41a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloaders\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "val_dataset = torch.utils.data.TensorDataset(X_val, y_val)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "97e9a4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMWithAttention(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim):\n",
    "        super(BiLSTMWithAttention, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.attention = nn.Linear(hidden_dim * 2, 1)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        outputs, _ = self.lstm(embedded)\n",
    "        attn_weights = torch.softmax(self.attention(outputs).squeeze(-1), dim=1)\n",
    "        context = torch.sum(outputs * attn_weights.unsqueeze(-1), dim=1)\n",
    "        return self.fc(context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96c319c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BiLSTMWithAttention(\n",
    "    vocab_size=len(vocab_dict),\n",
    "    embed_dim=EMBED_DIM,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    output_dim=len(le.classes_)\n",
    ").to(DEVICE)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ec260f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for inputs, labels in loader:\n",
    "        inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "47e649d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(inputs)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    print(classification_report(all_labels, all_preds, target_names=le.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e071ad9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "Train Loss: 1.7740\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\ASUS\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\ASUS\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry       0.45      0.67      0.54       875\n",
      "    confused       0.74      0.56      0.64       511\n",
      "   depressed       0.52      0.61      0.56      1109\n",
      "     excited       0.51      0.40      0.44       534\n",
      "        fear       0.78      0.73      0.75       873\n",
      "    grateful       0.43      0.18      0.25       502\n",
      "       happy       0.00      0.00      0.00       296\n",
      "     hopeful       0.68      0.71      0.70      1124\n",
      "      lonely       0.51      0.69      0.59       987\n",
      "      regret       0.62      0.33      0.43       573\n",
      "    rejected       0.96      0.41      0.58       303\n",
      "         sad       0.31      0.40      0.35       965\n",
      "\n",
      "    accuracy                           0.54      8652\n",
      "   macro avg       0.54      0.47      0.48      8652\n",
      "weighted avg       0.54      0.54      0.53      8652\n",
      "\n",
      "Epoch 2/2\n",
      "Train Loss: 1.2702\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry       0.58      0.62      0.60       875\n",
      "    confused       0.78      0.58      0.66       511\n",
      "   depressed       0.52      0.66      0.58      1109\n",
      "     excited       0.47      0.47      0.47       534\n",
      "        fear       0.77      0.76      0.76       873\n",
      "    grateful       0.38      0.39      0.39       502\n",
      "       happy       0.62      0.21      0.31       296\n",
      "     hopeful       0.71      0.74      0.72      1124\n",
      "      lonely       0.58      0.67      0.62       987\n",
      "      regret       0.61      0.45      0.52       573\n",
      "    rejected       0.83      0.55      0.66       303\n",
      "         sad       0.38      0.37      0.38       965\n",
      "\n",
      "    accuracy                           0.58      8652\n",
      "   macro avg       0.60      0.54      0.56      8652\n",
      "weighted avg       0.59      0.58      0.58      8652\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    loss = train(model, train_loader)\n",
    "    print(f\"Train Loss: {loss:.4f}\")\n",
    "    evaluate(model, val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7a679745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "torch.save(model.state_dict(), \"bilstm_model.pt\")\n",
    "\n",
    "# Save vocab and label encoder\n",
    "with open(\"vocab.pkl\", \"wb\") as f:\n",
    "    pickle.dump(vocab_dict, f)\n",
    "\n",
    "with open(\"label_encoder.pkl\", \"wb\") as f:\n",
    "    pickle.dump(le, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b10dc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
